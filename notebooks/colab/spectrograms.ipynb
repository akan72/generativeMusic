{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udpuaiWHkELx"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I7jHzNbBkFn2",
    "outputId": "bf9bd42d-64b5-4569-d1e7-e1bfcbca9ac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/data/audio\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content/drive/My Drive/data/audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rNo8vbuAk6o_",
    "outputId": "0f8d2781-4c3c-467a-91e9-32426cfee306"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from random import shuffle\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nsA0ahuljrX"
   },
   "outputs": [],
   "source": [
    "songs = (re.search('.+?(?=\\.wav)', x).group(0)\n",
    "              for x in glob.iglob('*.wav')) \n",
    "\n",
    "#build spectrograms for conv net\n",
    "spectrograms = []\n",
    "for song in songs:\n",
    "  sample_rate, samples = wavfile.read(song+'.wav')\n",
    "  frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "  spectrogram = spectrogram[:,10000:10500] #sample middle 2.5s of audio \n",
    "  #np.place(spectrogram, spectrogram == 0, 10**-10) #avoid div by 0 for log\n",
    "  with open('../annotations/meter/' + song + '.meter', 'r') as file:\n",
    "      meter = file.readline()\n",
    "\n",
    "  spectrograms.append([song, spectrogram, meter])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9hjB6MqqScx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data(spec_data, frac):\n",
    "    #split = math.ceil(len(spec_data) * frac)\n",
    "    split = 80\n",
    "    \n",
    "    random.shuffle(spec_data)\n",
    "    \n",
    "    x_train = np.array([x[1] for x in spec_data[:split]])\n",
    "    y_train = np.array([x[2] for x in spec_data[:split]])\n",
    "    \n",
    "    x_test = np.array([x[1] for x in spec_data[split:]])\n",
    "    y_test = np.array([x[2] for x in spec_data[split:]])\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UWJLZfFnqwpc",
    "outputId": "a927c1fb-f33f-4e66-ede3-c034782aa772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3    0.2875 0.2375 0.175 ]\n",
      "[0.15789473 0.13157895 0.28947368 0.42105263]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data(spectrograms, 0.7) #70% train-test split\n",
    "\n",
    "#height and width of img\n",
    "img_x, img_y = x_train.shape[1:]\n",
    "\n",
    "#reshape x_train, x_test to 4D tensor - (samples, size1,size2,channels)\n",
    "x_train = x_train.reshape((x_train.shape[0], img_x, img_y, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], img_x, img_y, 1))\n",
    "\n",
    "import cv2\n",
    "x_train = cv2.merge((x_train,x_train,x_train)).reshape((x_train.shape[0], img_x, img_y, 3))\n",
    "x_test = cv2.merge((x_test, x_test, x_test)).reshape((x_test.shape[0], img_x, img_y, 3))\n",
    "\n",
    "\n",
    "#x_train = x_train.reshape((80, 129, 500, 1))\n",
    "#x_train = cv2.merge((x_train, x_train, x_train))\n",
    "\n",
    "\n",
    "#Does not work, why?\n",
    "#stack greyscale img depth-wise to create img in rgb\n",
    "#x_train = np.dstack((x_train, x_train, x_train))\n",
    "#x_test = np.dstack((x_test, x_test, x_test))\n",
    "\n",
    "\n",
    "\n",
    "#process input for compatibility with InceptionV3 model\n",
    "x_train = preprocess_input(x_train)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 4)\n",
    "y_test = keras.utils.to_categorical(y_test, 4)\n",
    "\n",
    "\n",
    "#reshape y_train, y_test to (N,1) tensor for generator\n",
    "#y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "#y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "print(np.sum(y_train, axis = 0)/len(y_train))\n",
    "print(np.sum(y_test, axis = 0)/len(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJrY-duB2zoy"
   },
   "outputs": [],
   "source": [
    "#generator code for model.fit_generator()\n",
    "def generator(features, labels, batch_size):\n",
    "  # Create empty arrays to contain batch of features and labels\n",
    "\n",
    "  batch_features = np.zeros((batch_size, img_x, img_y, 3))\n",
    "  batch_labels = np.zeros((batch_size,1))\n",
    "\n",
    "  while True:\n",
    "   for i in range(batch_size):\n",
    "     # choose random index in features\n",
    "     index= np.random.choice(len(x_train))\n",
    "     batch_features[i] = features[index]\n",
    "     batch_labels[i] = labels[index]\n",
    "   yield batch_features, keras.utils.to_categorical(batch_labels, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6998
    },
    "colab_type": "code",
    "id": "Uqo11gos8RAS",
    "outputId": "3df86944-ead9-49ba-abe6-03109251f94a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 38 samples\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 34s 426ms/step - loss: 1.4603 - acc: 0.2625 - val_loss: 1.4682 - val_acc: 0.1842\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.4050 - acc: 0.2625 - val_loss: 1.5100 - val_acc: 0.2368\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.3517 - acc: 0.3500 - val_loss: 1.4930 - val_acc: 0.2368\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.3640 - acc: 0.3125 - val_loss: 1.4737 - val_acc: 0.2368\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.3251 - acc: 0.3750 - val_loss: 1.4954 - val_acc: 0.1316\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.3484 - acc: 0.2625 - val_loss: 1.4711 - val_acc: 0.2105\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.3009 - acc: 0.4000 - val_loss: 1.4967 - val_acc: 0.2368\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.2983 - acc: 0.4875 - val_loss: 1.4930 - val_acc: 0.2632\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.2589 - acc: 0.4625 - val_loss: 1.4426 - val_acc: 0.2632\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.2592 - acc: 0.4625 - val_loss: 1.4799 - val_acc: 0.1842\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.2686 - acc: 0.4000 - val_loss: 1.4237 - val_acc: 0.2632\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.2336 - acc: 0.4625 - val_loss: 1.4183 - val_acc: 0.2895\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.2098 - acc: 0.4625 - val_loss: 1.4907 - val_acc: 0.2632\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.1598 - acc: 0.5125 - val_loss: 1.4200 - val_acc: 0.2368\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.1645 - acc: 0.5375 - val_loss: 1.4817 - val_acc: 0.2368\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.1705 - acc: 0.5125 - val_loss: 1.4418 - val_acc: 0.1842\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.1175 - acc: 0.6125 - val_loss: 1.4173 - val_acc: 0.2105\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.0824 - acc: 0.6625 - val_loss: 1.4184 - val_acc: 0.3421\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.1146 - acc: 0.5125 - val_loss: 1.4209 - val_acc: 0.3158\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.0856 - acc: 0.6000 - val_loss: 1.5779 - val_acc: 0.3421\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.0460 - acc: 0.6625 - val_loss: 1.5829 - val_acc: 0.2632\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.0587 - acc: 0.5500 - val_loss: 1.6605 - val_acc: 0.2368\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.0755 - acc: 0.6000 - val_loss: 1.5878 - val_acc: 0.2632\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.9656 - acc: 0.7000 - val_loss: 1.4946 - val_acc: 0.3684\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.9717 - acc: 0.6750 - val_loss: 1.4511 - val_acc: 0.3158\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.9755 - acc: 0.6750 - val_loss: 1.4864 - val_acc: 0.3158\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.9128 - acc: 0.6875 - val_loss: 1.5422 - val_acc: 0.3158\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.9272 - acc: 0.7250 - val_loss: 1.5351 - val_acc: 0.2895\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.8694 - acc: 0.7250 - val_loss: 1.4698 - val_acc: 0.3158\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.8448 - acc: 0.7250 - val_loss: 1.5713 - val_acc: 0.3158\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.8614 - acc: 0.7000 - val_loss: 1.7063 - val_acc: 0.2632\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.7413 - acc: 0.8000 - val_loss: 1.6701 - val_acc: 0.2368\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.8547 - acc: 0.7000 - val_loss: 1.8041 - val_acc: 0.2632\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.7838 - acc: 0.7500 - val_loss: 1.8641 - val_acc: 0.3158\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.6696 - acc: 0.8125 - val_loss: 1.8435 - val_acc: 0.2895\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.6794 - acc: 0.8750 - val_loss: 1.7010 - val_acc: 0.2895\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.6354 - acc: 0.8750 - val_loss: 1.9585 - val_acc: 0.3158\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.6483 - acc: 0.7875 - val_loss: 2.0092 - val_acc: 0.2632\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.5515 - acc: 0.9000 - val_loss: 1.8110 - val_acc: 0.3158\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.5940 - acc: 0.8750 - val_loss: 1.7494 - val_acc: 0.2632\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.6637 - acc: 0.7750 - val_loss: 1.8252 - val_acc: 0.2632\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.5083 - acc: 0.9000 - val_loss: 1.8105 - val_acc: 0.2632\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.5227 - acc: 0.8625 - val_loss: 1.7353 - val_acc: 0.3158\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.5900 - acc: 0.7625 - val_loss: 1.8037 - val_acc: 0.2632\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.5124 - acc: 0.8625 - val_loss: 1.7003 - val_acc: 0.2632\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.4660 - acc: 0.8875 - val_loss: 1.6792 - val_acc: 0.2368\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.4809 - acc: 0.8625 - val_loss: 1.6586 - val_acc: 0.3421\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.4512 - acc: 0.9250 - val_loss: 1.7381 - val_acc: 0.2895\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.4302 - acc: 0.9250 - val_loss: 1.6922 - val_acc: 0.3421\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.3412 - acc: 0.9500 - val_loss: 1.7971 - val_acc: 0.3947\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.4064 - acc: 0.9125 - val_loss: 1.7881 - val_acc: 0.3158\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2684 - acc: 0.9500 - val_loss: 1.7444 - val_acc: 0.3684\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.4895 - acc: 0.8250 - val_loss: 1.7999 - val_acc: 0.3684\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2984 - acc: 0.9500 - val_loss: 1.9035 - val_acc: 0.2632\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2719 - acc: 0.9625 - val_loss: 1.9350 - val_acc: 0.2368\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.3017 - acc: 0.9625 - val_loss: 1.9288 - val_acc: 0.2895\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.4272 - acc: 0.8875 - val_loss: 1.8469 - val_acc: 0.3684\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2943 - acc: 0.9500 - val_loss: 1.8651 - val_acc: 0.3421\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2085 - acc: 0.9750 - val_loss: 1.7438 - val_acc: 0.4211\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2642 - acc: 0.9500 - val_loss: 1.6865 - val_acc: 0.3684\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2084 - acc: 0.9875 - val_loss: 1.7541 - val_acc: 0.3158\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2519 - acc: 0.9375 - val_loss: 1.8354 - val_acc: 0.2895\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.2332 - acc: 0.9500 - val_loss: 2.0134 - val_acc: 0.2632\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2667 - acc: 0.9500 - val_loss: 1.9758 - val_acc: 0.2632\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2902 - acc: 0.9250 - val_loss: 1.8943 - val_acc: 0.2895\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2052 - acc: 0.9625 - val_loss: 1.8594 - val_acc: 0.3421\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2002 - acc: 0.9625 - val_loss: 1.6731 - val_acc: 0.4474\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2549 - acc: 0.9500 - val_loss: 1.6964 - val_acc: 0.4474\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1855 - acc: 0.9875 - val_loss: 1.8587 - val_acc: 0.4211\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1706 - acc: 0.9625 - val_loss: 1.8031 - val_acc: 0.3684\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2184 - acc: 0.9375 - val_loss: 1.7664 - val_acc: 0.3947\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2759 - acc: 0.9125 - val_loss: 1.6971 - val_acc: 0.5000\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2208 - acc: 0.9500 - val_loss: 1.7928 - val_acc: 0.4737\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1763 - acc: 0.9875 - val_loss: 1.6521 - val_acc: 0.3947\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.2249 - acc: 0.9625 - val_loss: 1.7097 - val_acc: 0.3947\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1484 - acc: 0.9750 - val_loss: 1.7444 - val_acc: 0.4737\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1752 - acc: 0.9250 - val_loss: 1.8577 - val_acc: 0.4737\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2087 - acc: 0.9375 - val_loss: 1.9324 - val_acc: 0.4211\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1723 - acc: 0.9375 - val_loss: 1.8704 - val_acc: 0.3421\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1607 - acc: 0.9625 - val_loss: 1.7838 - val_acc: 0.3947\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1711 - acc: 0.9500 - val_loss: 1.8392 - val_acc: 0.3158\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.2093 - acc: 0.9625 - val_loss: 1.6911 - val_acc: 0.4211\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1206 - acc: 0.9875 - val_loss: 1.7502 - val_acc: 0.3684\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.3578 - acc: 0.8875 - val_loss: 1.8158 - val_acc: 0.3947\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1412 - acc: 0.9750 - val_loss: 1.9217 - val_acc: 0.3421\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2280 - acc: 0.9000 - val_loss: 1.9662 - val_acc: 0.3421\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1326 - acc: 1.0000 - val_loss: 1.8840 - val_acc: 0.4211\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2117 - acc: 0.9250 - val_loss: 1.8911 - val_acc: 0.3947\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1855 - acc: 0.9750 - val_loss: 1.8667 - val_acc: 0.3421\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1443 - acc: 0.9625 - val_loss: 1.8119 - val_acc: 0.3684\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2067 - acc: 0.9625 - val_loss: 1.7687 - val_acc: 0.3947\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0840 - acc: 0.9875 - val_loss: 1.7646 - val_acc: 0.4211\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1606 - acc: 0.9500 - val_loss: 1.7550 - val_acc: 0.4211\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2275 - acc: 0.9250 - val_loss: 1.7494 - val_acc: 0.3684\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1579 - acc: 0.9625 - val_loss: 1.8388 - val_acc: 0.3947\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2699 - acc: 0.9125 - val_loss: 1.9810 - val_acc: 0.3947\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1016 - acc: 1.0000 - val_loss: 2.0278 - val_acc: 0.3684\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1426 - acc: 0.9500 - val_loss: 1.9361 - val_acc: 0.3421\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1607 - acc: 0.9625 - val_loss: 1.9634 - val_acc: 0.3947\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0990 - acc: 0.9875 - val_loss: 2.3322 - val_acc: 0.2895\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1156 - acc: 0.9875 - val_loss: 2.2710 - val_acc: 0.3421\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2026 - acc: 0.9500 - val_loss: 2.3153 - val_acc: 0.2895\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1147 - acc: 0.9875 - val_loss: 2.1651 - val_acc: 0.2895\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1570 - acc: 0.9750 - val_loss: 2.1521 - val_acc: 0.3158\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2041 - acc: 0.9500 - val_loss: 2.2350 - val_acc: 0.2632\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2431 - acc: 0.9375 - val_loss: 2.2767 - val_acc: 0.2895\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1050 - acc: 0.9750 - val_loss: 2.1113 - val_acc: 0.3158\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1295 - acc: 0.9875 - val_loss: 2.1025 - val_acc: 0.3421\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1587 - acc: 0.9625 - val_loss: 2.0891 - val_acc: 0.3684\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1163 - acc: 0.9875 - val_loss: 2.0755 - val_acc: 0.3421\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1187 - acc: 0.9750 - val_loss: 2.0971 - val_acc: 0.3158\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0906 - acc: 0.9750 - val_loss: 2.1595 - val_acc: 0.2632\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0827 - acc: 1.0000 - val_loss: 2.1529 - val_acc: 0.3158\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0690 - acc: 1.0000 - val_loss: 2.2048 - val_acc: 0.2632\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1445 - acc: 0.9750 - val_loss: 2.1858 - val_acc: 0.3158\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1559 - acc: 0.9500 - val_loss: 2.0891 - val_acc: 0.3684\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1913 - acc: 0.9500 - val_loss: 2.0712 - val_acc: 0.4211\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1357 - acc: 0.9500 - val_loss: 2.0531 - val_acc: 0.3684\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2491 - acc: 0.9250 - val_loss: 2.0796 - val_acc: 0.3158\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0847 - acc: 0.9875 - val_loss: 2.0429 - val_acc: 0.3158\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1011 - acc: 0.9875 - val_loss: 2.0921 - val_acc: 0.2895\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0836 - acc: 0.9875 - val_loss: 2.3856 - val_acc: 0.2105\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1281 - acc: 0.9625 - val_loss: 2.4033 - val_acc: 0.2368\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1344 - acc: 0.9750 - val_loss: 2.2500 - val_acc: 0.2632\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0919 - acc: 0.9750 - val_loss: 2.1349 - val_acc: 0.2895\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0759 - acc: 0.9875 - val_loss: 2.1924 - val_acc: 0.2632\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1753 - acc: 0.9625 - val_loss: 2.1820 - val_acc: 0.2895\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0871 - acc: 0.9875 - val_loss: 2.1533 - val_acc: 0.3158\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1075 - acc: 0.9500 - val_loss: 2.1447 - val_acc: 0.3421\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1024 - acc: 0.9750 - val_loss: 2.3475 - val_acc: 0.2895\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1892 - acc: 0.9375 - val_loss: 2.2851 - val_acc: 0.3421\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0978 - acc: 0.9875 - val_loss: 2.2279 - val_acc: 0.3947\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2642 - acc: 0.9125 - val_loss: 2.3063 - val_acc: 0.2632\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1260 - acc: 0.9625 - val_loss: 2.3909 - val_acc: 0.2895\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0833 - acc: 1.0000 - val_loss: 2.4080 - val_acc: 0.3158\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1213 - acc: 0.9750 - val_loss: 2.5303 - val_acc: 0.3158\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1287 - acc: 0.9750 - val_loss: 2.6090 - val_acc: 0.3158\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0618 - acc: 1.0000 - val_loss: 2.5900 - val_acc: 0.3158\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0690 - acc: 0.9750 - val_loss: 2.6802 - val_acc: 0.3158\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0994 - acc: 0.9875 - val_loss: 2.7182 - val_acc: 0.2632\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1758 - acc: 0.9625 - val_loss: 2.6817 - val_acc: 0.3158\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0604 - acc: 0.9750 - val_loss: 2.4422 - val_acc: 0.3158\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0938 - acc: 0.9625 - val_loss: 2.5311 - val_acc: 0.2632\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0426 - acc: 1.0000 - val_loss: 2.5943 - val_acc: 0.2632\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0559 - acc: 1.0000 - val_loss: 2.6396 - val_acc: 0.3158\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0766 - acc: 0.9875 - val_loss: 2.5537 - val_acc: 0.2895\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0828 - acc: 0.9875 - val_loss: 2.4858 - val_acc: 0.3158\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0751 - acc: 0.9875 - val_loss: 2.3798 - val_acc: 0.3158\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0395 - acc: 1.0000 - val_loss: 2.4248 - val_acc: 0.2895\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1432 - acc: 0.9625 - val_loss: 2.5611 - val_acc: 0.2632\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1488 - acc: 0.9500 - val_loss: 2.4100 - val_acc: 0.3158\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0501 - acc: 1.0000 - val_loss: 2.2853 - val_acc: 0.3684\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.2680 - acc: 0.8875 - val_loss: 2.3707 - val_acc: 0.3421\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0558 - acc: 0.9875 - val_loss: 2.3603 - val_acc: 0.3158\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0472 - acc: 0.9875 - val_loss: 2.3871 - val_acc: 0.3421\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1740 - acc: 0.9375 - val_loss: 2.4053 - val_acc: 0.3158\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0647 - acc: 0.9875 - val_loss: 2.3920 - val_acc: 0.3158\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0738 - acc: 0.9875 - val_loss: 2.4035 - val_acc: 0.3421\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1576 - acc: 0.9500 - val_loss: 2.3663 - val_acc: 0.2895\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0591 - acc: 1.0000 - val_loss: 2.3147 - val_acc: 0.2895\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0929 - acc: 0.9625 - val_loss: 2.2704 - val_acc: 0.2895\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0965 - acc: 0.9750 - val_loss: 2.6129 - val_acc: 0.2895\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0823 - acc: 0.9750 - val_loss: 2.7288 - val_acc: 0.3158\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0334 - acc: 1.0000 - val_loss: 2.6432 - val_acc: 0.2368\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0256 - acc: 1.0000 - val_loss: 2.5893 - val_acc: 0.2105\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0922 - acc: 0.9750 - val_loss: 2.5379 - val_acc: 0.3158\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0639 - acc: 1.0000 - val_loss: 2.5859 - val_acc: 0.2895\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0531 - acc: 1.0000 - val_loss: 2.5209 - val_acc: 0.2895\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0415 - acc: 1.0000 - val_loss: 2.5073 - val_acc: 0.3158\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0752 - acc: 0.9625 - val_loss: 2.4694 - val_acc: 0.3158\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0519 - acc: 0.9875 - val_loss: 2.4427 - val_acc: 0.2632\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0804 - acc: 0.9500 - val_loss: 2.3847 - val_acc: 0.2895\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1053 - acc: 0.9625 - val_loss: 2.3043 - val_acc: 0.3158\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0375 - acc: 1.0000 - val_loss: 2.5090 - val_acc: 0.3684\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0308 - acc: 1.0000 - val_loss: 2.4876 - val_acc: 0.3684\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0422 - acc: 0.9875 - val_loss: 2.4794 - val_acc: 0.3947\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1128 - acc: 0.9625 - val_loss: 2.8863 - val_acc: 0.3158\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0739 - acc: 0.9625 - val_loss: 2.6557 - val_acc: 0.3158\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0386 - acc: 0.9875 - val_loss: 2.7032 - val_acc: 0.3421\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0480 - acc: 0.9875 - val_loss: 2.7434 - val_acc: 0.3684\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0256 - acc: 1.0000 - val_loss: 2.7087 - val_acc: 0.3421\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0909 - acc: 0.9750 - val_loss: 2.5487 - val_acc: 0.3947\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.2008 - acc: 0.9250 - val_loss: 2.4897 - val_acc: 0.4211\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0605 - acc: 0.9875 - val_loss: 2.5638 - val_acc: 0.3421\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0229 - acc: 1.0000 - val_loss: 2.5256 - val_acc: 0.3684\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0178 - acc: 1.0000 - val_loss: 2.4906 - val_acc: 0.3684\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0535 - acc: 0.9875 - val_loss: 2.7016 - val_acc: 0.3421\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0411 - acc: 1.0000 - val_loss: 2.6420 - val_acc: 0.3421\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 2.5801 - val_acc: 0.3421\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0859 - acc: 0.9750 - val_loss: 2.6431 - val_acc: 0.2632\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1114 - acc: 0.9625 - val_loss: 2.6520 - val_acc: 0.3421\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.1308 - acc: 0.9750 - val_loss: 2.2959 - val_acc: 0.3947\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0717 - acc: 0.9750 - val_loss: 2.2836 - val_acc: 0.3421\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.0777 - acc: 0.9750 - val_loss: 2.3165 - val_acc: 0.3421\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 0.1706 - acc: 0.9625 - val_loss: 2.5551 - val_acc: 0.3421\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0379 - acc: 0.9875 - val_loss: 2.5624 - val_acc: 0.3158\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0790 - acc: 0.9625 - val_loss: 2.5478 - val_acc: 0.3158\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0668 - acc: 0.9875 - val_loss: 2.5362 - val_acc: 0.3684\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0536 - acc: 0.9750 - val_loss: 2.4160 - val_acc: 0.3947\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0452 - acc: 0.9875 - val_loss: 2.5584 - val_acc: 0.3158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe21bfa96d8>"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "51CkS5ntufe-",
    "outputId": "fb8ff61f-b210-41e5-a080-599338404bd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob = model.predict(x_train) \n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "XgOe_E4qu40S",
    "outputId": "b34240cb-81fe-4fd3-84f6-0a0e67acf2a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 0, 1, 1, 1, 1, 3, 2, 3, 2, 3, 2, 1, 2, 3, 0, 0, 1, 0, 3,\n",
       "       3, 3, 0, 3, 0, 3, 2, 3, 3, 1, 2, 1, 2, 3, 2, 0, 3, 2, 1, 3, 2, 2,\n",
       "       3, 3, 2, 1, 1, 3, 2, 3, 0, 3, 0, 3, 1, 1, 2, 0, 3, 3, 3, 2, 0, 0,\n",
       "       1, 3, 1, 0, 2, 1, 3, 2, 2, 0, 0, 1, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(len(y_train),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_b86Zy4rPrJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(generator(x_train, y_train, 5), steps_per_epoch=16, epochs=20)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(generator(x_train, y_train, 5), steps_per_epoch=16, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "InX-BOzSXQZS",
    "outputId": "28cd83bd-f1d5-4faa-aa75-ed0510417b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 13.99729397422389\n",
      "Test accuracy: 0.13157894736842105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#score = model.evaluate(x_test, keras.utils.to_categorical(y_test, num_classes=4), verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "bANRm_8VNruj",
    "outputId": "c142a628-178d-4291-a748-6c9182882722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 223,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "le.transform(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgCLiiJLQnSZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5418
    },
    "colab_type": "code",
    "id": "4IHffJ5PTmda",
    "outputId": "9c4630ba-8fe6-4bc8-d751-b8411634348c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv2d_1\n",
      "2 batch_normalization_1\n",
      "3 activation_1\n",
      "4 conv2d_2\n",
      "5 batch_normalization_2\n",
      "6 activation_2\n",
      "7 conv2d_3\n",
      "8 batch_normalization_3\n",
      "9 activation_3\n",
      "10 max_pooling2d_1\n",
      "11 conv2d_4\n",
      "12 batch_normalization_4\n",
      "13 activation_4\n",
      "14 conv2d_5\n",
      "15 batch_normalization_5\n",
      "16 activation_5\n",
      "17 max_pooling2d_2\n",
      "18 conv2d_9\n",
      "19 batch_normalization_9\n",
      "20 activation_9\n",
      "21 conv2d_7\n",
      "22 conv2d_10\n",
      "23 batch_normalization_7\n",
      "24 batch_normalization_10\n",
      "25 activation_7\n",
      "26 activation_10\n",
      "27 average_pooling2d_1\n",
      "28 conv2d_6\n",
      "29 conv2d_8\n",
      "30 conv2d_11\n",
      "31 conv2d_12\n",
      "32 batch_normalization_6\n",
      "33 batch_normalization_8\n",
      "34 batch_normalization_11\n",
      "35 batch_normalization_12\n",
      "36 activation_6\n",
      "37 activation_8\n",
      "38 activation_11\n",
      "39 activation_12\n",
      "40 mixed0\n",
      "41 conv2d_16\n",
      "42 batch_normalization_16\n",
      "43 activation_16\n",
      "44 conv2d_14\n",
      "45 conv2d_17\n",
      "46 batch_normalization_14\n",
      "47 batch_normalization_17\n",
      "48 activation_14\n",
      "49 activation_17\n",
      "50 average_pooling2d_2\n",
      "51 conv2d_13\n",
      "52 conv2d_15\n",
      "53 conv2d_18\n",
      "54 conv2d_19\n",
      "55 batch_normalization_13\n",
      "56 batch_normalization_15\n",
      "57 batch_normalization_18\n",
      "58 batch_normalization_19\n",
      "59 activation_13\n",
      "60 activation_15\n",
      "61 activation_18\n",
      "62 activation_19\n",
      "63 mixed1\n",
      "64 conv2d_23\n",
      "65 batch_normalization_23\n",
      "66 activation_23\n",
      "67 conv2d_21\n",
      "68 conv2d_24\n",
      "69 batch_normalization_21\n",
      "70 batch_normalization_24\n",
      "71 activation_21\n",
      "72 activation_24\n",
      "73 average_pooling2d_3\n",
      "74 conv2d_20\n",
      "75 conv2d_22\n",
      "76 conv2d_25\n",
      "77 conv2d_26\n",
      "78 batch_normalization_20\n",
      "79 batch_normalization_22\n",
      "80 batch_normalization_25\n",
      "81 batch_normalization_26\n",
      "82 activation_20\n",
      "83 activation_22\n",
      "84 activation_25\n",
      "85 activation_26\n",
      "86 mixed2\n",
      "87 conv2d_28\n",
      "88 batch_normalization_28\n",
      "89 activation_28\n",
      "90 conv2d_29\n",
      "91 batch_normalization_29\n",
      "92 activation_29\n",
      "93 conv2d_27\n",
      "94 conv2d_30\n",
      "95 batch_normalization_27\n",
      "96 batch_normalization_30\n",
      "97 activation_27\n",
      "98 activation_30\n",
      "99 max_pooling2d_3\n",
      "100 mixed3\n",
      "101 conv2d_35\n",
      "102 batch_normalization_35\n",
      "103 activation_35\n",
      "104 conv2d_36\n",
      "105 batch_normalization_36\n",
      "106 activation_36\n",
      "107 conv2d_32\n",
      "108 conv2d_37\n",
      "109 batch_normalization_32\n",
      "110 batch_normalization_37\n",
      "111 activation_32\n",
      "112 activation_37\n",
      "113 conv2d_33\n",
      "114 conv2d_38\n",
      "115 batch_normalization_33\n",
      "116 batch_normalization_38\n",
      "117 activation_33\n",
      "118 activation_38\n",
      "119 average_pooling2d_4\n",
      "120 conv2d_31\n",
      "121 conv2d_34\n",
      "122 conv2d_39\n",
      "123 conv2d_40\n",
      "124 batch_normalization_31\n",
      "125 batch_normalization_34\n",
      "126 batch_normalization_39\n",
      "127 batch_normalization_40\n",
      "128 activation_31\n",
      "129 activation_34\n",
      "130 activation_39\n",
      "131 activation_40\n",
      "132 mixed4\n",
      "133 conv2d_45\n",
      "134 batch_normalization_45\n",
      "135 activation_45\n",
      "136 conv2d_46\n",
      "137 batch_normalization_46\n",
      "138 activation_46\n",
      "139 conv2d_42\n",
      "140 conv2d_47\n",
      "141 batch_normalization_42\n",
      "142 batch_normalization_47\n",
      "143 activation_42\n",
      "144 activation_47\n",
      "145 conv2d_43\n",
      "146 conv2d_48\n",
      "147 batch_normalization_43\n",
      "148 batch_normalization_48\n",
      "149 activation_43\n",
      "150 activation_48\n",
      "151 average_pooling2d_5\n",
      "152 conv2d_41\n",
      "153 conv2d_44\n",
      "154 conv2d_49\n",
      "155 conv2d_50\n",
      "156 batch_normalization_41\n",
      "157 batch_normalization_44\n",
      "158 batch_normalization_49\n",
      "159 batch_normalization_50\n",
      "160 activation_41\n",
      "161 activation_44\n",
      "162 activation_49\n",
      "163 activation_50\n",
      "164 mixed5\n",
      "165 conv2d_55\n",
      "166 batch_normalization_55\n",
      "167 activation_55\n",
      "168 conv2d_56\n",
      "169 batch_normalization_56\n",
      "170 activation_56\n",
      "171 conv2d_52\n",
      "172 conv2d_57\n",
      "173 batch_normalization_52\n",
      "174 batch_normalization_57\n",
      "175 activation_52\n",
      "176 activation_57\n",
      "177 conv2d_53\n",
      "178 conv2d_58\n",
      "179 batch_normalization_53\n",
      "180 batch_normalization_58\n",
      "181 activation_53\n",
      "182 activation_58\n",
      "183 average_pooling2d_6\n",
      "184 conv2d_51\n",
      "185 conv2d_54\n",
      "186 conv2d_59\n",
      "187 conv2d_60\n",
      "188 batch_normalization_51\n",
      "189 batch_normalization_54\n",
      "190 batch_normalization_59\n",
      "191 batch_normalization_60\n",
      "192 activation_51\n",
      "193 activation_54\n",
      "194 activation_59\n",
      "195 activation_60\n",
      "196 mixed6\n",
      "197 conv2d_65\n",
      "198 batch_normalization_65\n",
      "199 activation_65\n",
      "200 conv2d_66\n",
      "201 batch_normalization_66\n",
      "202 activation_66\n",
      "203 conv2d_62\n",
      "204 conv2d_67\n",
      "205 batch_normalization_62\n",
      "206 batch_normalization_67\n",
      "207 activation_62\n",
      "208 activation_67\n",
      "209 conv2d_63\n",
      "210 conv2d_68\n",
      "211 batch_normalization_63\n",
      "212 batch_normalization_68\n",
      "213 activation_63\n",
      "214 activation_68\n",
      "215 average_pooling2d_7\n",
      "216 conv2d_61\n",
      "217 conv2d_64\n",
      "218 conv2d_69\n",
      "219 conv2d_70\n",
      "220 batch_normalization_61\n",
      "221 batch_normalization_64\n",
      "222 batch_normalization_69\n",
      "223 batch_normalization_70\n",
      "224 activation_61\n",
      "225 activation_64\n",
      "226 activation_69\n",
      "227 activation_70\n",
      "228 mixed7\n",
      "229 conv2d_73\n",
      "230 batch_normalization_73\n",
      "231 activation_73\n",
      "232 conv2d_74\n",
      "233 batch_normalization_74\n",
      "234 activation_74\n",
      "235 conv2d_71\n",
      "236 conv2d_75\n",
      "237 batch_normalization_71\n",
      "238 batch_normalization_75\n",
      "239 activation_71\n",
      "240 activation_75\n",
      "241 conv2d_72\n",
      "242 conv2d_76\n",
      "243 batch_normalization_72\n",
      "244 batch_normalization_76\n",
      "245 activation_72\n",
      "246 activation_76\n",
      "247 max_pooling2d_4\n",
      "248 mixed8\n",
      "249 conv2d_81\n",
      "250 batch_normalization_81\n",
      "251 activation_81\n",
      "252 conv2d_78\n",
      "253 conv2d_82\n",
      "254 batch_normalization_78\n",
      "255 batch_normalization_82\n",
      "256 activation_78\n",
      "257 activation_82\n",
      "258 conv2d_79\n",
      "259 conv2d_80\n",
      "260 conv2d_83\n",
      "261 conv2d_84\n",
      "262 average_pooling2d_8\n",
      "263 conv2d_77\n",
      "264 batch_normalization_79\n",
      "265 batch_normalization_80\n",
      "266 batch_normalization_83\n",
      "267 batch_normalization_84\n",
      "268 conv2d_85\n",
      "269 batch_normalization_77\n",
      "270 activation_79\n",
      "271 activation_80\n",
      "272 activation_83\n",
      "273 activation_84\n",
      "274 batch_normalization_85\n",
      "275 activation_77\n",
      "276 mixed9_0\n",
      "277 concatenate_1\n",
      "278 activation_85\n",
      "279 mixed9\n",
      "280 conv2d_90\n",
      "281 batch_normalization_90\n",
      "282 activation_90\n",
      "283 conv2d_87\n",
      "284 conv2d_91\n",
      "285 batch_normalization_87\n",
      "286 batch_normalization_91\n",
      "287 activation_87\n",
      "288 activation_91\n",
      "289 conv2d_88\n",
      "290 conv2d_89\n",
      "291 conv2d_92\n",
      "292 conv2d_93\n",
      "293 average_pooling2d_9\n",
      "294 conv2d_86\n",
      "295 batch_normalization_88\n",
      "296 batch_normalization_89\n",
      "297 batch_normalization_92\n",
      "298 batch_normalization_93\n",
      "299 conv2d_94\n",
      "300 batch_normalization_86\n",
      "301 activation_88\n",
      "302 activation_89\n",
      "303 activation_92\n",
      "304 activation_93\n",
      "305 batch_normalization_94\n",
      "306 activation_86\n",
      "307 mixed9_1\n",
      "308 concatenate_2\n",
      "309 activation_94\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A49GJ2ozUP1c",
    "outputId": "fabe082f-b6c1-40ce-9c00-8ad3794f0d6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2, 4, 5]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "random.shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "IIjDY-TxUQwe",
    "outputId": "6eb2b55b-9f66-4129-f8d9-5a7de115b8b5"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-6149a2aba43c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'google.colab.drive' has no attribute 'unmount'"
     ]
    }
   ],
   "source": [
    "drive.unmount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1267
    },
    "colab_type": "code",
    "id": "alyPFtd0k1Mj",
    "outputId": "d2b36d0c-ca9b-47c3-f5f1-aa7ec4cf25c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       13335212 kB\n",
      "MemFree:        11191004 kB\n",
      "MemAvailable:   12657464 kB\n",
      "Buffers:           44948 kB\n",
      "Cached:          1596884 kB\n",
      "SwapCached:            0 kB\n",
      "Active:           514624 kB\n",
      "Inactive:        1406360 kB\n",
      "Active(anon):     257460 kB\n",
      "Inactive(anon):      328 kB\n",
      "Active(file):     257164 kB\n",
      "Inactive(file):  1406032 kB\n",
      "Unevictable:           0 kB\n",
      "Mlocked:               0 kB\n",
      "SwapTotal:             0 kB\n",
      "SwapFree:              0 kB\n",
      "Dirty:               232 kB\n",
      "Writeback:             0 kB\n",
      "AnonPages:        279132 kB\n",
      "Mapped:           154400 kB\n",
      "Shmem:               844 kB\n",
      "Slab:             119596 kB\n",
      "SReclaimable:      89848 kB\n",
      "SUnreclaim:        29748 kB\n",
      "KernelStack:        3468 kB\n",
      "PageTables:         4416 kB\n",
      "NFS_Unstable:          0 kB\n",
      "Bounce:                0 kB\n",
      "WritebackTmp:          0 kB\n",
      "CommitLimit:     6667604 kB\n",
      "Committed_AS:    1675276 kB\n",
      "VmallocTotal:   34359738367 kB\n",
      "VmallocUsed:           0 kB\n",
      "VmallocChunk:          0 kB\n",
      "AnonHugePages:         0 kB\n",
      "ShmemHugePages:        0 kB\n",
      "ShmemPmdMapped:        0 kB\n",
      "HugePages_Total:       0\n",
      "HugePages_Free:        0\n",
      "HugePages_Rsvd:        0\n",
      "HugePages_Surp:        0\n",
      "Hugepagesize:       2048 kB\n",
      "DirectMap4k:       69620 kB\n",
      "DirectMap2M:     5173248 kB\n",
      "DirectMap1G:    10485760 kB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 12722042531394066010, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 3389415551423531816\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 8339090031054887459\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11281553818\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 13549150098634449447\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat /proc/meminfo\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "swe2tTSeptLY",
    "outputId": "2abeffdd-2087-47ea-8001-462a80b2f5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "#input_tensor = Input(shape=(224, 224, 3))  # this assumes K.image_data_format() == 'channels_last'\n",
    "\n",
    "#model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)\n",
    "\n",
    "# this could also be the output a different Keras model or layer\n",
    "#input_tensor = Input(shape=(img_x, img_y, 3))  \n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3( weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RgwidgymqUcA",
    "outputId": "f5451688-db17-465b-89bd-53bdd19525a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(224, 224, 3))  # this assumes K.image_data_format() == 'channels_last'\n",
    "\n",
    "model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U2DVjfzyqa7H",
    "outputId": "2c1fc446-2a90-4a5c-bbff-64de10f1cff0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(?, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "Iofkq_Pkqtbk",
    "outputId": "25797ddb-d5c2-4f08-9412-84d9ec5b6798"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02_12003_2-02_Pakkala_Nilabadi',\n",
       " array([[1.83138561e+01, 5.14330673e+00, 2.04259014e+01, ...,\n",
       "         1.17135666e+02, 1.27727928e+02, 1.67983398e+01],\n",
       "        [1.90238983e+02, 1.69976208e+03, 3.05827734e+03, ...,\n",
       "         9.73505005e+02, 1.40374890e+03, 3.33984888e+03],\n",
       "        [1.89349350e+02, 2.73411279e+03, 1.44540820e+03, ...,\n",
       "         5.16861670e+03, 3.34138281e+03, 5.96172998e+03],\n",
       "        ...,\n",
       "        [3.30927001e-08, 8.70707299e-06, 4.53788607e-06, ...,\n",
       "         4.01475472e-06, 4.88131218e-06, 4.22224548e-06],\n",
       "        [4.80746121e-06, 1.98490488e-06, 2.52803766e-05, ...,\n",
       "         8.97796872e-06, 1.24984990e-06, 2.03112941e-05],\n",
       "        [1.07506629e-07, 1.28157330e-06, 8.33216109e-06, ...,\n",
       "         6.85883642e-13, 1.70183739e-06, 7.59690238e-06]], dtype=float32),\n",
       " '7/8']"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrograms[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "colab_type": "code",
    "id": "tDOm0E9i63y3",
    "outputId": "b8093dcb-8be4-41cb-e0e7-eda83b6fe58f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  4.7833514 ,   4.7833514 ,   4.7833514 ],\n",
       "        [ -0.21646124,  -0.21646124,  -0.21646124],\n",
       "        [  4.0584393 ,   4.0584393 ,   4.0584393 ],\n",
       "        ...,\n",
       "        [  1.1657901 ,   1.1657901 ,   1.1657901 ],\n",
       "        [  1.6616268 ,   1.6616268 ,   1.6616268 ],\n",
       "        [ -0.9548845 ,  -0.9548845 ,  -0.9548845 ]],\n",
       "\n",
       "       [[133.93564   , 133.93564   , 133.93564   ],\n",
       "        [ 50.45263   ,  50.45263   ,  50.45263   ],\n",
       "        [101.830086  , 101.830086  , 101.830086  ],\n",
       "        ...,\n",
       "        [ 69.55197   ,  69.55197   ,  69.55197   ],\n",
       "        [107.47329   , 107.47329   , 107.47329   ],\n",
       "        [ 30.32808   ,  30.32808   ,  30.32808   ]],\n",
       "\n",
       "       [[ 58.485153  ,  58.485153  ,  58.485153  ],\n",
       "        [  3.8016324 ,   3.8016324 ,   3.8016324 ],\n",
       "        [ 67.432205  ,  67.432205  ,  67.432205  ],\n",
       "        ...,\n",
       "        [  4.039701  ,   4.039701  ,   4.039701  ],\n",
       "        [ 90.98996   ,  90.98996   ,  90.98996   ],\n",
       "        [ 26.159327  ,  26.159327  ,  26.159327  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -1.        ,  -1.        ,  -1.        ],\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        ...,\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        [ -1.        ,  -1.        ,  -1.        ],\n",
       "        [ -1.        ,  -1.        ,  -1.        ]],\n",
       "\n",
       "       [[ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        [ -0.9999999 ,  -0.9999999 ,  -0.9999999 ],\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        ...,\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        [ -1.        ,  -1.        ,  -1.        ],\n",
       "        [ -1.        ,  -1.        ,  -1.        ]],\n",
       "\n",
       "       [[ -1.        ,  -1.        ,  -1.        ],\n",
       "        [ -1.        ,  -1.        ,  -1.        ],\n",
       "        [ -1.        ,  -1.        ,  -1.        ],\n",
       "        ...,\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994],\n",
       "        [ -0.99999994,  -0.99999994,  -0.99999994]]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcWTC_V-7BQu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Spectrograms.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
